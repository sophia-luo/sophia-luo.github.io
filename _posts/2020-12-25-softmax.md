---
layout: page
title: "Should we still use softmax as the final layer?"
subheadline: "Machine Learning"
teaser: "Softmax layer is commonly used as the last layer to generate probabilities, but it can lead to instability. Why?"
header: no
image:
    title: SoftmaxLayer.png
    thumb:  SoftmaxLayer.png
    homepage: SoftmaxLayer.png
    caption: Softmax layer
    caption_url: "https://developers.google.com/machine-learning/crash-course/images/SoftmaxLayer.svg?dcb_=0.482934043191239"
categories:
    - machine learning
---

In [tensorflow beginner tutorial][4]:
> <span class="teaser">Note: It is possible to bake this tf.nn.softmax in as the activation function for the last layer of the network. While this can make the model output more directly interpretable, this approach is discouraged as it's impossible to provide an exact and numerically stable loss calculation for all models when using a softmax output. </span><cite>[tensorflow][4]</cite>

### Explaination
This is also a question I stumble upon at the begining. Let's still use DeepMind's Simon Osindero's [slide][1] to explain:
[![][2]][2]
The grey block on the left we are looking at is only a cross entropy operation, the input $x$ (a vector) could be the softmax output from previous layer (not the input for the neutral network), and $y$ (a scalar) is the cross entropy result of $x$. To propagate the gradient back, we need to calculate the gradient of $dy/dx_i$, which is $-p_i/x_i$ for each element in $x$. 
As we know the softmax function scale the logits into the range [0,1], so if in one training step, the neutral network becomes super confident and predict one of the probabilties $x_i$ to be 0 then we have a numerical problem in calculting $dy/dx_i$.

While in the other case where we take the logits and calculate the softmax and crossentropy at one shot (XentLogits function), we don't have this problem. Because the derivative of XentLogits is $dy/dx_i = y - p_i$, a more elaborated derivation can be found [here][3].


[1]: https://docs.google.com/presentation/d/e/2PACX-1vQwrivdqqBR8teLQ7prKtiDyMLSqgGBzTxfQ6BKXPVvpFpLRUQOmqTm57LEMIy3IIK14RTLcBcT-PCO/pub?start=false&loop=false&delayms=60000&slide=id.g1a727d4a2c_0_814
[2]: https://i.stack.imgur.com/lV7Ty.jpg
[3]: http://machinelearningmechanic.com/deep_learning/2019/09/04/cross-entropy-loss-derivative.html
[4]: https://www.tensorflow.org/tutorials/quickstart/beginner

