---
layout: page
title: "What are your dreams? When are you planning to go for them?"
subheadline: "Courage, Dream"
teaser: "Everyone has big or small dreams. However, only a few of us have the courage to make them happen. What about you?"
header: no
image:
    title: Makeyourdreamcomestrue.png
    thumb:  SoftmaxLayer-thumb.png
    homepage: Makeyourdreamcomestrue.png
    caption: Makeyourdreamcomestrue
    caption_url: "https://www.quora.com/How-can-one-answer-the-question-what-are-your-goals-or-dreams-in-your-life"
comments: true
categories:
    - Coach
---

> ##<span class="teaser">Insanity is doing the same thing over and over again and expecting different results. </span><cite>Albert Einstein</cite>

### Listen to your inner voice
Life is short, and you are already naked. There is no reason not to follow your heart.
[![live your dream][1]]

### You are not alone


The grey block on the left we are looking at is only a cross entropy operation, the input *x* (a vector) could be the softmax output from previous layer (not the input for the neutral network), and *y* (a scalar) is the cross entropy result of *x*. To propagate the gradient back, we need to calculate the gradient of 
![formula](https://render.githubusercontent.com/render/math?math=dy/dx_i), which is ![formula](https://render.githubusercontent.com/render/math?math=-p_i/x_i) for each element in *x*. 
As we know the softmax function scale the logits into the range [0,1], so if in one training step, the neutral network becomes super confident and predict one of the probabilties 
![formula](https://render.githubusercontent.com/render/math?math=x_i) to be 0, then we have a numerical problem in calculting 
![formula](https://render.githubusercontent.com/render/math?math=dy/dx_i).


[![CE+Softmax][5]][5]
While in the other case, where we take the logits and calculate the softmax and crossentropy at one shot (XentLogits function), we don't have this problem. Because the derivative of XentLogits is 
![formula](https://render.githubusercontent.com/render/math?math=dy/dx_i = y - p_i)
a more elaborated derivation can be found [here][3].

### In Practice
We would still need to use Softmax function in the end, in order to calculate the cross-entropy loss, 
but not as the final layer in the neutral network, rather embed it into the loss function. 
Still using the previous example, in tensorflow you can do:
``` python
model = tf.keras.models.Sequential([
    tf.keras.layers.Flatten(input_shape=(28, 28)),
    tf.keras.layers.Dense(128, activation='relu'),
    tf.keras.layers.Dropout(0.2),
    tf.keras.layers.Dense(10),
])
loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)
model.compile(optimizer='adam', loss=loss_fn, metrics=['accuracy'])
```
We remove the Softmax layer from the model, but for `SparseCategoricalCrossentropy` function, we pass `from_logits=True`, and the Softmax will be
calculated automagically before the cross-entropy is performed.

[1]: https://www.google.de/url?sa=i&url=https%3A%2F%2Fwww.goalcast.com%2F2016%2F05%2F19%2Fhow-to-live-your-dream-life%2F&psig=AOvVaw0hPz8_UCJA3Wkc-BL9yTOX&ust=1609428674274000&source=images&cd=vfe&ved=0CAIQjRxqFwoTCIiw8qOD9u0CFQAAAAAdAAAAABAr
[2]: https://i.stack.imgur.com/lV7Ty.jpg
[3]: http://machinelearningmechanic.com/deep_learning/2019/09/04/cross-entropy-loss-derivative.html
[4]: https://www.tensorflow.org/tutorials/quickstart/beginner
[5]: https://i.stack.imgur.com/uGw1c.jpg

